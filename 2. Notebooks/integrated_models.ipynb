{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IN0uLIHVE65t"
      },
      "outputs": [],
      "source": [
        "!pip -q install diffusers transformers scipy mediapy deep_translator gtts langdetect pytextrank sentence-transformers sklearn wget simpletransformers sentencepiece speechrecognition pydub firebase_admin"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "privateKey={\n",
        "  \"type\": \"service_account\",\n",
        "  \"project_id\": \"art-in-our-world\",\n",
        "  \"private_key_id\": \"d728e27af8920d8613e34a5967ff64bce1245e0d\",\n",
        "  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEvAIBADANBgkqhkiG9w0BAQEFAASCBKYwggSiAgEAAoIBAQCxyzybo2hQKYLC\\nEcN8P4qM3Wt01HdHdQVPJLiqPMnyZyGnAW6qqEngIWwu2byEo5WAyaELqabIxyUV\\nUbX2fygHNJE6X6LGv+JFbBD2VaBcNMG6yXB6BuRpPrxrLrvlRtfoY6mIx7vZ0iqN\\nuuyIEG6dwAQyyuzL+l5JZ92UDdqJrfvFR8NY6s/vcUta4IwK/9FtFOxd+n1smItF\\nLjNwBFg41MLGI9srwXvB+Xnt9dy3hEKzQUmHXxxKWFZs1/xK9Db+vlyGjDSzLL/1\\n2TwFA/Kz8VIfUj3eiVlTHtW4m25XV+1PqJNNv0GfQdGE9cAfc/huiYo4x5AVWdgG\\na5BZ0nblAgMBAAECggEAFKn7bFdsq0x3zxiDcPBtqPufHqAHEP4P1Of+soKIMi08\\nYw2ukBUpKOyZh0R1PbFJkgNdPV7FbLaAn960WGbk/xlj+7Xb1F++67SosAi5r53O\\nGmiJMxWT/It3GLoAyqAWPgM/VA9qzOE5Yumjzhii0qDbnA+rv/VkegGhZjDKApBN\\niEeJ/2v5iOHVoPe+71p3F8pg5E15tNGLKILcNg3vKWz50rOJLRMt5QXEyO56sA/u\\nLb31yojDM5vQU6o9ZkkiyRKrQUdbnAYWoPKIbzSUewSK+UcT52YxM31WEBu0gcOy\\ngxcaAyyg/q/OwZrBVWgQnKwmAGmpI0J2OmtoslLGTQKBgQDVdbu9Lvu30PgZOqUe\\ndKFTgZM3gChu+Pi23gQQF0N/N0TCh0inlrPWxw2t/ySH9AHOPQcsc7sud1N6QEdu\\nTSCbHCj+K6UoV9sKubHO34COhfgRywpHnVVRE8KN4g1P4l4NpiTJjoqkwJ8U2+4v\\n/Kc83FKjEVMec1oYKK3QiknK3wKBgQDVOeZ9gsN4nYlpa16NgztsMwthOW0pbglu\\nGGJor1vg/jJs0EU9+wLbYusyuCkY2q3WCjPx+Svi7Ch0RcCdB9uSZnvQj3JLU8Rt\\nYK58/DUPKYlVX9/eYFJtn39zYWbgZh0v/rFs+giLr1Igqrn2PBgMx4bE2qLOS6u+\\nfgBTKWN6uwKBgEoyTjuIJT7TiClc1SAs8haQp5dXGLbaG7mUmlzteSyzDVeApqR6\\nIbC6J+sfEJehHS+OzvuIjrusBnwr86hHbtn7g9Q1X4cSYrTrrv51eb4QmxW0ObKC\\n4bD46VtgA7iMupyL76WPwmlZ+vHJMJXFgGwLOT4tO3MOLXbQTPp71wKJAoGAdTLM\\nA7S6KDO+8Zi4cePt53zvBlOAMsNun3r7rDqmr7ZIlRKs3HJ5kYh/anNa83RPFFSr\\nVJb2jlXrrm0gf69v+rol1+7gjZLPZqc7gStHJrxX5fs6m4oqBGMktJCW8URvSpAd\\niZ6sz8pfw+DTMgq51VxeMRlTXzyse5naJ9uZ+D0CgYA5ZwV4vMPIewu5jqwAdKRR\\nSqxd2ObOQpm2bkB3jLuF6B+K4FVnCCuw5ZBiZ8rReuZM7KXlLyjAXgdDEmr+JWrq\\nrI3gaHuD2qzyGSacyMFNumgySdAetTkycXkTeNqyZTh2+VrQJH5lW9uSDMcaUOBk\\nlDCETYwOAXWCxVMrBhFYyQ==\\n-----END PRIVATE KEY-----\\n\",\n",
        "  \"client_email\": \"firebase-adminsdk-98mqz@art-in-our-world.iam.gserviceaccount.com\",\n",
        "  \"client_id\": \"117015720897995139866\",\n",
        "  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
        "  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
        "  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
        "  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/firebase-adminsdk-98mqz%40art-in-our-world.iam.gserviceaccount.com\"\n",
        "}"
      ],
      "metadata": {
        "id": "1gBNvpeqrwfQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import firebase_admin\n",
        "from firebase_admin import credentials, firestore\n",
        "cred = credentials.Certificate(privateKey)\n",
        "firebase_admin.initialize_app(cred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yR64S5UIr4D1",
        "outputId": "43b6df60-e737-4618-fbdb-e3776d00f0b9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<firebase_admin.App at 0x7fba6f8a2710>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRND9mlCHKav",
        "outputId": "350a47f8-24f5-48e7-8548-e1bcd613e0d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Mp-Bq0NVFfkq"
      },
      "outputs": [],
      "source": [
        "from deep_translator import GoogleTranslator\n",
        "def translateSentence(sentence,lang='en'):\n",
        "  return GoogleTranslator('auto',lang).translate(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nO1QLft99tvP"
      },
      "outputs": [],
      "source": [
        "import speech_recognition as sr\n",
        "from os import path\n",
        "from pydub import AudioSegment\n",
        "def speechToText(fileName):\n",
        "  if 'mp3' in fileName:\n",
        "    src=fileName\n",
        "    dst='output.wav'\n",
        "    sound = AudioSegment.from_mp3(src)\n",
        "    sound.export(dst, format=\"wav\")\n",
        "    fileName=dst\n",
        "  r = sr.Recognizer()\n",
        "  with sr.AudioFile(fileName) as source:\n",
        "    audio = r.record(source)\n",
        "  return r.recognize_google(audio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XcHIJqmOFlHV"
      },
      "outputs": [],
      "source": [
        "from gtts import gTTS\n",
        "from langdetect import detect, DetectorFactory\n",
        "def TextToSpeech(text):\n",
        "  DetectorFactory.seed = 0\n",
        "  language=detect(text)\n",
        "  print(language)\n",
        "  try:\n",
        "    ttsObj = gTTS(text=text, lang=language, slow=False)\n",
        "  except:\n",
        "     ttsObj = gTTS(text=text, lang='ar', slow=False)\n",
        "  ttsObj.save(f\"test.mp3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NgKyOe6vFnYv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk import sent_tokenize\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "prIENJhRTRuT"
      },
      "outputs": [],
      "source": [
        "from transformers import PegasusForConditionalGeneration\n",
        "from transformers import PegasusTokenizer\n",
        "from transformers import pipeline\n",
        "model_name = \"google/pegasus-xsum\"\n",
        "pegasus_tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "# Define PEGASUS model\n",
        "pegasus_model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
        "def summarization(text):\n",
        "# Create tokens\n",
        "  tokens = pegasus_tokenizer(text, truncation=True, padding=\"longest\", return_tensors=\"pt\")\n",
        "  encoded_summary = pegasus_model.generate(**tokens)\n",
        "# Decode summarized text\n",
        "  decoded_summary = pegasus_tokenizer.decode(\n",
        "      encoded_summary[0],\n",
        "      skip_special_tokens=True\n",
        ")\n",
        "  summarizer = pipeline(\n",
        "    \"summarization\", \n",
        "    model=model_name, \n",
        "    tokenizer=pegasus_tokenizer, \n",
        "    framework=\"pt\"\n",
        ")\n",
        "  summary = summarizer(text, min_length=30, max_length=150)\n",
        "  return summary[0][\"summary_text\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-e_xmN2LFq2n"
      },
      "outputs": [],
      "source": [
        "def process_bert_similarity(base_document,documents):\n",
        "  # This will download and load the pretrained model offered by UKPLab.\n",
        "  model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "\t# Although it is not explicitly stated in the official document of sentence transformer, the original BERT is meant for a shorter sentence. We will feed the model by sentences instead of the whole documents.\n",
        "  sentences = sent_tokenize(base_document)\n",
        "  base_embeddings_sentences = model.encode(sentences)\n",
        "  base_embeddings = np.mean(np.array(base_embeddings_sentences), axis=0)\n",
        "  \n",
        "  vectors = []\n",
        "  for i, document in enumerate(documents):\n",
        "    sentences = sent_tokenize(summarization(document))\n",
        "    embeddings_sentences = model.encode(sentences)\n",
        "    embeddings = np.mean(np.array(embeddings_sentences), axis=0)\n",
        "    vectors.append(embeddings)\n",
        "    #print(\"making vector at index:\", i)\n",
        "  scores = cosine_similarity([base_embeddings], vectors).flatten()\n",
        "\n",
        "  highest_score = 0\n",
        "  highest_score_index = 0\n",
        "  score_dict={}\n",
        "  for i, score in enumerate(scores):\n",
        "        score_dict[score]=i\n",
        "  score_dict=sorted(score_dict.items(), key=lambda x: x[0])\n",
        "  return  score_dict[-10:]\n",
        "  most_similar_document = documents[highest_score_index]\n",
        "  print(\"---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\n\")\n",
        "  print(f\"Most similar document: {most_similar_document} \\n\\n\\nThe Score: {highest_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "OIrm2g9vHbfq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('APOD.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "IH_UrC5zIgTi"
      },
      "outputs": [],
      "source": [
        "docs=data.explanation.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def flow(text):\n",
        "  DetectorFactory.seed = 0\n",
        "  language=detect(text)\n",
        "  eng_text=translateSentence(text)\n",
        "  scores_dict=process_bert_similarity(eng_text,docs[:5])\n",
        "  TextToSpeech(translateSentence(docs[scores_dict[-1][1]],language))\n",
        "  return scores_dict[-1][1]"
      ],
      "metadata": {
        "id": "xxK50u0WxCHK"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "XOzvqqp8PFZo"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "  time.sleep(5)\n",
        "  db = firestore.client()\n",
        "  docs1 = db.collection(u'search').where(u'result', u'==', None).stream()\n",
        "  for doc in docs1:\n",
        "    text=doc.to_dict()['searchKeyword']\n",
        "    result=data.loc[flow(text)].to_dict()\n",
        "    print(result)\n",
        "    doc_id=doc.id\n",
        "    doc_ref = db.collection(u'search').document(doc_id).set({\n",
        "        u'searchKeyword': text,\n",
        "        u'result': result\n",
        "    })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aL2OqHpw1_y",
        "outputId": "3e0e26af-ac9d-4168-9ed8-8497a4b3a33e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:1232: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 64 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  UserWarning,\n",
            "Your max_length is set to 150, but you input_length is only 148. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=74)\n",
            "Your max_length is set to 150, but you input_length is only 149. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=74)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "en\n",
            "{'Unnamed: 0': 2, 'date': '1995-06-21', 'explanation': \"Today's Picture: June 21, 1995    The Aftermath of the Great Supernova in 1987  Picture Credit: Hubble Space Telescope  Explanation:  In 1987 a star in one of the Milky Way's satellite galaxies exploded. In 1994 the Hubble Space Telescope, in orbit around the earth, took a very detailed picture of the remnants of this explosion. This picture, above, showed unusual and unexpected rings, and astronomers are not sure how they formed.  For more information see HST press release.  We keep an archive of previous Astronomy Pictures of the Day.   Astronomy Picture of the Day is brought to you by  Robert Nemiroff and  Jerry Bonnell . Original material on this page is copyrighted to Robert J. Nemiroff and Jerry T. Bonnell.\", 'hdurl': 'https://apod.nasa.gov/apod/image/sn1987a_hst.gif', 'media_type': 'image', 'service_version': 'v1', 'title': 'Supernova 1987a Aftermath', 'url': 'https://apod.nasa.gov/apod/image/sn1987a_hst.gif', 'copyright': nan}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSwVUBrFfjBV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}